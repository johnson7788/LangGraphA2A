#!/usr/bin/env python
# -*- coding: utf-8 -*-
# ç”¨äºå’ŒrabbitMQè¿›è¡Œäº¤äº’
import os
import random
import string
import requests
import time
import json
import asyncio
import traceback
import aiohttp
import dotenv
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from mq_handler import start_consumer, MQHandler
from A2Aclient import A2AClientWrapper
from Parse_QA import QAParser
dotenv.load_dotenv()

IMAGE_API = os.environ.get('IMAGE_API')
parser_message = QAParser(
    base_url=IMAGE_API,
    timeout=30.0,
)

# åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± ï¼Œå®šä¹‰çº¿ç¨‹æ± çš„å¤§å°10
executor = ThreadPoolExecutor(max_workers=20)

RABBITMQ_HOST = os.environ["RABBITMQ_HOST"]
RABBITMQ_PORT = os.environ["RABBITMQ_PORT"]
RABBITMQ_USERNAME = os.environ["RABBITMQ_USERNAME"]
RABBITMQ_PASSWORD = os.environ["RABBITMQ_PASSWORD"]
RABBITMQ_VIRTUAL_HOST = os.environ["RABBITMQ_VIRTUAL_HOST"]
QUEUE_NAME_ANSWER = os.environ["QUEUE_NAME_ANSWER"]
QUEUE_NAME_QUESTION = os.environ["QUEUE_NAME_QUESTION"]
AGENT_URL = os.environ["AGENT_URL"]
ENTITY_URL = os.environ["ENTITY_URL"]

def entity_indentify_extract_match_db(content):
    """entity_indentify_extractè¯†åˆ«æ¥å£
    eg: content: è¿‘å¹´æ¥ï¼Œç³–å°¿ç—…æ‚£è€…æ•°é‡é€å¹´ä¸Šå‡ï¼Œå¸¸ç”¨çš„æ²»ç–—è¯ç‰©åŒ…æ‹¬ç›é…¸äºŒç”²åŒèƒç‰‡ã€èƒ°å²›ç´ æ³¨å°„æ¶²ç­‰ã€‚éƒ¨åˆ†æ‚£è€…è¿˜ä¼´æœ‰é«˜è¡€å‹ï¼Œéœ€è¦æœç”¨æ°¯æ²™å¦æˆ–ç¡è‹¯åœ°å¹³è¿›è¡Œæ§åˆ¶ã€‚"
    Returns:
        eg:
{
    "code": 0,
    "msg": "success",
    "data": {
        "diseases": [
            {
                "id": 4633,
                "disease_name": "é«˜è¡€å‹",
                "overview": "é«˜è¡€å‹ï¼ˆhypertensionï¼‰æ˜¯æŒ‡ä»¥ä½“å¾ªç¯åŠ¨è„‰è¡€å‹ï¼ˆæ”¶ç¼©å‹å’Œ/æˆ–èˆ’å¼ å‹ï¼‰å¢é«˜ä¸ºä¸»è¦ç‰¹å¾ï¼ˆæ”¶ç¼©å‹â‰¥140æ¯«ç±³æ±æŸ±ï¼Œèˆ’å¼ å‹â‰¥90æ¯«ç±³æ±æŸ±ï¼‰ï¼Œå¯ä¼´æœ‰å¿ƒã€è„‘ã€è‚¾ç­‰å™¨å®˜çš„åŠŸèƒ½æˆ–å™¨è´¨æ€§æŸå®³çš„ä¸´åºŠç»¼åˆå¾ã€‚é«˜è¡€å‹æ˜¯æœ€å¸¸è§çš„æ…¢æ€§ç—…ï¼Œä¹Ÿæ˜¯å¿ƒè„‘è¡€ç®¡ç—…æœ€ä¸»è¦çš„å±é™©å› ç´ ã€‚æ­£å¸¸äººçš„è¡€å‹éšå†…å¤–ç¯å¢ƒå˜åŒ–åœ¨ä¸€å®šèŒƒå›´å†…æ³¢åŠ¨ã€‚åœ¨æ•´ä½“äººç¾¤ï¼Œè¡€å‹æ°´å¹³éšå¹´é¾„é€æ¸å‡é«˜ï¼Œä»¥æ”¶ç¼©å‹æ›´ä¸ºæ˜æ˜¾ï¼Œä½†50å²åèˆ’å¼ å‹å‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œè„‰å‹ä¹Ÿéšä¹‹åŠ å¤§ã€‚è¿‘å¹´æ¥ï¼Œäººä»¬å¯¹å¿ƒè¡€ç®¡ç—…å¤šé‡å±é™©å› ç´ çš„ä½œç”¨ä»¥åŠå¿ƒã€è„‘ã€è‚¾é¶å™¨å®˜ä¿æŠ¤çš„è®¤è¯†ä¸æ–­æ·±å…¥ï¼Œé«˜è¡€å‹çš„è¯Šæ–­æ ‡å‡†ä¹Ÿåœ¨ä¸æ–­è°ƒæ•´ï¼Œç›®å‰è®¤ä¸ºåŒä¸€è¡€å‹æ°´å¹³çš„æ‚£è€…å‘ç”Ÿå¿ƒè¡€ç®¡ç—…çš„å±é™©ä¸åŒï¼Œå› æ­¤æœ‰äº†è¡€å‹åˆ†å±‚çš„æ¦‚å¿µï¼Œå³å‘ç”Ÿå¿ƒè¡€ç®¡ç—…å±é™©åº¦ä¸åŒçš„æ‚£è€…ï¼Œé€‚å®œè¡€å‹æ°´å¹³åº”æœ‰ä¸åŒã€‚è¡€å‹å€¼å’Œå±é™©å› ç´ è¯„ä¼°æ˜¯è¯Šæ–­å’Œåˆ¶å®šé«˜è¡€å‹æ²»ç–—æ–¹æ¡ˆçš„ä¸»è¦ä¾æ®ï¼Œä¸åŒæ‚£è€…é«˜è¡€å‹ç®¡ç†çš„ç›®æ ‡ä¸åŒï¼ŒåŒ»ç”Ÿé¢å¯¹æ‚£è€…æ—¶åœ¨å‚è€ƒæ ‡å‡†çš„åŸºç¡€ä¸Šï¼Œæ ¹æ®å…¶å…·ä½“æƒ…å†µåˆ¤æ–­è¯¥æ‚£è€…æœ€åˆé€‚çš„è¡€å‹èŒƒå›´ï¼Œé‡‡ç”¨é’ˆå¯¹æ€§çš„æ²»ç–—æªæ–½ã€‚åœ¨æ”¹å–„ç”Ÿæ´»æ–¹å¼çš„åŸºç¡€ä¸Šï¼Œæ¨èä½¿ç”¨24å°æ—¶é•¿æ•ˆé™å‹è¯ç‰©æ§åˆ¶è¡€å‹ã€‚é™¤è¯„ä¼°è¯Šå®¤è¡€å‹å¤–ï¼Œæ‚£è€…è¿˜åº”æ³¨æ„å®¶åº­æ¸…æ™¨è¡€å‹çš„ç›‘æµ‹å’Œç®¡ç†ï¼Œä»¥æ§åˆ¶è¡€å‹ï¼Œé™ä½å¿ƒè„‘è¡€ç®¡äº‹ä»¶çš„å‘ç”Ÿç‡ã€‚",
                "match_word": "é«˜è¡€å‹"
            },
            {
                "id": 7608,
                "disease_name": "ç³–å°¿ç—…",
                "overview": "ç³–å°¿ç—…æ˜¯ä¸€ç»„ä»¥é«˜è¡€ç³–ä¸ºç‰¹å¾çš„ä»£è°¢æ€§ç–¾ç—…ã€‚é«˜è¡€ç³–åˆ™æ˜¯ç”±äºèƒ°å²›ç´ åˆ†æ³Œç¼ºé™·æˆ–å…¶ç”Ÿç‰©ä½œç”¨å—æŸï¼Œæˆ–ä¸¤è€…å…¼æœ‰å¼•èµ·ã€‚ç³–å°¿ç—…æ—¶é•¿æœŸå­˜åœ¨çš„é«˜è¡€ç³–ï¼Œå¯¼è‡´å„ç§ç»„ç»‡ï¼Œç‰¹åˆ«æ˜¯çœ¼ã€è‚¾ã€å¿ƒè„ã€è¡€ç®¡ã€ç¥ç»çš„æ…¢æ€§æŸå®³ã€åŠŸèƒ½éšœç¢ã€‚",
                "match_word": "ç³–å°¿ç—…"
            }
        ],
        "drugs": [
            {
                "id": 259152,
                "drug_id": "185917",
                "med_name": "ç›é…¸äºŒç”²åŒèƒç‰‡",
                "component": "<p>ã€åŒ–å­¦åç§°ã€‘ <br/>\n1,1-äºŒç”²åŸºåŒèƒç›é…¸ç›</p>\n\n<p>ã€åŒ–å­¦ç»“æ„å¼ã€‘</p>\n\n<p><img rel=\"https://img1.dxycdn.com/2019/1211/311/3384551398150199955-73.gif\"  src = \"https://img1.dxycdn.com/2019/1211/311/3384551398150199955-73.gif\" alt = \"\" /></p>\n\n<p>ã€åˆ†å­é‡ã€‘ <br/>\n165.63</p>",
                "match_word": "ç›é…¸äºŒç”²åŒèƒç‰‡"
            },
            {
                "id": 236868,
                "drug_id": "177275",
                "med_name": "èƒ°å²›ç´ æ³¨å°„æ¶²",
                "component": "æœ¬å“ä¸»è¦æˆä»½ä¸ºèƒ°å²›ç´ ï¼ˆçŒªæˆ–ç‰›ï¼‰çš„ç­èŒæ°´æº¶æ¶²ã€‚è¾…æ–™ä¸ºï¼šç”˜æ²¹ã€‚",
                "match_word": "èƒ°å²›ç´ æ³¨å°„æ¶²"
            }
        ]
    }
}

    """
    url = f"{ENTITY_URL}/api/entity_indentify"
    start_time = time.time()
    headers = {'content-type': 'application/json'}
    data = {"match_db": True,"content": content}
    r = requests.post(url, data=json.dumps(data), headers=headers)
    assert r.status_code == 200, f"è¿”å›çš„status codeä¸æ˜¯200ï¼Œè¯·æ£€æŸ¥"
    # æ£€æŸ¥è½¬æ¢ç»“æœ
    res = r.json()
    print(json.dumps(res, indent=4, ensure_ascii=False))
    msg = res.get("msg")
    assert msg == "success", f"æ¥å£è¿”å›çš„msgä¸æ˜¯æˆåŠŸï¼Œè¯·æ£€æŸ¥"
    print(f"èŠ±è´¹æ—¶é—´: {time.time() - start_time}ç§’")
    return res

async def entity_indentify_extract_match_db_async(content):
    """å¼‚æ­¥ç‰ˆæœ¬çš„å®ä½“è¯†åˆ«æ¥å£"""
    url = f"{ENTITY_URL}/api/entity_indentify"
    start_time = time.time()
    headers = {'content-type': 'application/json'}
    data = {"match_db": True, "content": content}

    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=data, headers=headers) as resp:
            assert resp.status == 200, f"è¿”å›çš„status codeä¸æ˜¯200ï¼Œè¯·æ£€æŸ¥"
            res = await resp.json()
            print(json.dumps(res, indent=4, ensure_ascii=False))
            msg = res.get("msg")
            assert msg == "success", f"æ¥å£è¿”å›çš„msgä¸æ˜¯æˆåŠŸï¼Œè¯·æ£€æŸ¥"
            print(f"èŠ±è´¹æ—¶é—´: {time.time() - start_time}ç§’")
            return res

def call_tool_mapper(one_chunk_data):
    """
    å·¥å…·è°ƒç”¨æ—¶ï¼Œæ”¹æˆå‰ç«¯éœ€è¦çš„æ•°æ®æ ¼å¼
    """
    print(f"è¿›è¡Œtoolçš„è½¬æ¢: {one_chunk_data}")
    assert isinstance(one_chunk_data, dict), f"å¿…é¡»æ˜¯å­—å…¸æ ¼å¼: {one_chunk_data}"
    func_name = one_chunk_data["function"]["name"]
    func_args = one_chunk_data["function"]["arguments"]
    id = one_chunk_data["id"]
    mapper = {
        "search_document_db": {
            "name": "æ–‡çŒ®åº“",
            "globalization": "å›½å†…"
        },
        "search_guideline_db": {
            "name": "æŒ‡å—",
            "globalization": "å›½é™…"
        },
        "search_personal_db": {
            "name": "ä¸ªäººçŸ¥è¯†åº“",
            "globalization": "ä¸ªäºº"
        }
    }
    db_name = mapper[func_name]["name"]
    globalization = mapper[func_name]["globalization"]
    data = [
        {"status": "Working",
         "display": f"æ­£åœ¨æ£€ç´¢{db_name}\n",
         "name": db_name,
         "globalization": globalization,
         "func_name": func_name,
         "arguments": func_args,
         "id": id
         }
    ]
    print(f"è½¬æ¢çš„ç»“æœæ˜¯: {data}")
    return data
def result_tool_mapper(one_chunk_data):
    """
    å·¥å…·ç»“æœæ—¶ï¼Œæ”¹æˆå‰ç«¯éœ€è¦çš„æ•°æ®æ ¼å¼
    """
    print(f"è¿›è¡Œtoolçš„è½¬æ¢: {one_chunk_data}")
    assert isinstance(one_chunk_data, dict), f"å¿…é¡»æ˜¯å­—å…¸æ ¼å¼: {one_chunk_data}"
    func_name = one_chunk_data["name"]
    func_output = one_chunk_data["tool_output"]
    id = one_chunk_data["tool_call_id"]
    mapper = {
        "search_document_db": {
            "name": "æ–‡çŒ®åº“",
            "globalization": "å›½å†…"
        },
        "search_guideline_db": {
            "name": "æŒ‡å—",
            "globalization": "å›½é™…"
        },
        "search_personal_db": {
            "name": "ä¸ªäººçŸ¥è¯†åº“",
            "globalization": "ä¸ªäºº"
        }
    }
    db_name = mapper[func_name]["name"]
    globalization = mapper[func_name]["globalization"]
    data = [
        {"status": "Done",
         "display": f"æ£€ç´¢{db_name}å®Œæˆ\n",
         "name": db_name,
         "globalization": globalization,
         "func_name": func_name,
         "func_output": func_output,
         "id": id
         }
    ]
    print(f"è½¬æ¢çš„ç»“æœæ˜¯: {data}")
    return data
def metadata_tool_mapper(one_chunk_data):
    """
    metadataä¿¡æ¯å‘é€ç»™å‰ç«¯
    """
    print(f"è¿›è¡Œmetadataçš„è½¬æ¢: {one_chunk_data}")
    assert isinstance(one_chunk_data, dict), f"å¿…é¡»æ˜¯å­—å…¸æ ¼å¼: {one_chunk_data}"
    search_dbs = one_chunk_data["search_dbs"]
    data = []
    for search_db_res in search_dbs:
        func_name = search_db_res["db"]
        result = search_db_res["result"]
        mapper = {
            "search_document_db": {
                "name": "æ–‡çŒ®åº“",
                "globalization": "å›½å†…"
            },
            "search_guideline_db": {
                "name": "æŒ‡å—",
                "globalization": "å›½é™…"
            },
            "search_personal_db": {
                "name": "ä¸ªäººçŸ¥è¯†åº“",
                "globalization": "ä¸ªäºº"
            }
        }
        db_name = mapper[func_name]["name"]
        globalization = mapper[func_name]["globalization"]
        one_data = {
             "globalization": globalization,
             "name": db_name,
             "data": result,
             }
        data.append(one_data)
    print(f"è½¬æ¢çš„ç»“æœæ˜¯: {data}")
    return data


def handle_gpt_stream_response(session_id, user_id, function_id, stream_response):
    """
    å¤„ç†GPTæµå¼å“åº”
    """
    mq_handler = MQHandler(RABBITMQ_HOST, RABBITMQ_PORT, RABBITMQ_USERNAME, RABBITMQ_PASSWORD, RABBITMQ_VIRTUAL_HOST,
                           QUEUE_NAME_ANSWER)
    # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œå…ˆå¤„ç†é”™è¯¯ï¼šstream_responseæ˜¯å­—ç¬¦ä¸²å°±æ˜¯é”™è¯¯ï¼Œåº”è¯¥é»˜è®¤æ˜¯ç”Ÿæˆå™¨
    def send_error_message(error_msg):
        """å‘é€é”™è¯¯ä¿¡æ¯åˆ°æ¶ˆæ¯é˜Ÿåˆ—"""
        mq_handler.send_message({
            "sessionId": session_id,
            "userId": user_id,
            "functionId": function_id,
            "message": f'å‘ç”Ÿé”™è¯¯ï¼š{error_msg}',
            "reasoningMessage": "",
            "type": 4,
        })
        time.sleep(0.01)
        mq_handler.send_message({
            "sessionId": session_id,
            "userId": user_id,
            "functionId": function_id,
            "message": '[stop]',
            "reasoningMessage": "",
            "type": 4,
        })

    if isinstance(stream_response, str):
        send_error_message(stream_response)
        mq_handler.close_connection()
        return
    async def consume():
        try:
            # è®°å½•æ‰€æœ‰åœæ­¢çš„tools
            running_tools = []  # æ­£åœ¨æ£€ç´¢çš„å·¥å…·ï¼Œä¹Ÿåªå‘é€ç»™å‰ç«¯ä¸€æ¬¡
            stopped_tools = []
            reponse_content = ""
            async for chunk in stream_response:
                print(f"chunk: {chunk}")
                try:
                    data_type = chunk.get("type")
                    if data_type == "final":
                        print(f"data_typeæ˜¯finalï¼Œå¼€å§‹æœ€ç»ˆçš„stopè¿”å›")
                        answer_queue_message = {
                            "sessionId": session_id,
                            "userId": user_id,
                            "functionId": function_id,
                            "message": '[stop]',
                            "reasoningMessage": "",
                            "type": 4,
                        }
                    elif data_type == "reasoning":
                        answer_queue_message = {
                            "sessionId": session_id,
                            "userId": user_id,
                            "functionId": function_id,
                            "message": '',
                            "reasoningMessage": chunk.get("reasoning", ""),
                            "type": 4,
                        }
                    elif data_type == "text":
                        answer_queue_message = {
                            "sessionId": session_id,
                            "userId": user_id,
                            "functionId": function_id,
                            "message": chunk.get("text", ""),
                            "reasoningMessage": '',
                            "type": 4,
                        }
                        reponse_content += chunk.get("text", "")
                    elif data_type == "metadata":
                        metadata = chunk.get("data", {})
                        metadata_to_front = metadata_tool_mapper(metadata)
                        answer_queue_message = {
                            "sessionId": session_id,
                            "userId": user_id,
                            "functionId": function_id,
                            "message": json.dumps(metadata_to_front, ensure_ascii=False),
                            "reasoningMessage": '',
                            "type": 6,
                        }
                        print(f"[Info] å‘é€å¼•ç”¨æ•°æ®å®Œæˆtype6ï¼š{answer_queue_message}")
                    elif data_type == "tool_call":
                        chunk_data = chunk.get("data", {})
                        print(f"è§¦å‘äº†å‡½æ•°è°ƒç”¨: {chunk_data}")
                        tool_to_fronts = []
                        for one_chunk_data in chunk_data:
                            # åªå¤„ç†å‡½æ•°çš„è¿”å›ç»“æœï¼Œä¸å¤„ç†å‡½æ•°çš„è°ƒç”¨è¯·æ±‚
                            tool_to_front = call_tool_mapper(one_chunk_data)
                            # æ¯ä¸ªå·¥å…·åªåœæ­¢ä¸€æ¬¡
                            tool_name = tool_to_front[0]["name"]
                            if tool_name in running_tools:
                                continue
                            running_tools.append(tool_name)
                            tool_to_fronts.extend(tool_to_front)
                        if tool_to_fronts:
                            answer_queue_message = {
                                "sessionId": session_id,
                                "userId": user_id,
                                "functionId": function_id,
                                "message": json.dumps(tool_to_fronts, ensure_ascii=False),
                                "reasoningMessage": "",
                                "type": 5,
                            }
                            mq_handler.send_message(answer_queue_message)
                            print(f"[Info] å‘é€å·¥å…·ä½¿ç”¨çŠ¶æ€type5ï¼š{answer_queue_message}")
                        continue
                    elif data_type == "tool_result":
                        chunk_data = chunk.get("data", {})
                        print(f"è§¦å‘äº†å‡½æ•°ç»“æœè¿”å›: {chunk_data}")
                        for one_chunk_data in chunk_data:
                            # åªå¤„ç†å‡½æ•°çš„è¿”å›ç»“æœï¼Œä¸å¤„ç†å‡½æ•°çš„è°ƒç”¨è¯·æ±‚
                            tool_to_front = result_tool_mapper(one_chunk_data)
                            # æ¯ä¸ªå·¥å…·åªåœæ­¢ä¸€æ¬¡
                            tool_name = tool_to_front[0]["name"]
                            if tool_name in stopped_tools:
                                continue
                            stopped_tools.append(tool_name)
                            answer_queue_message = {
                                "sessionId": session_id,
                                "userId": user_id,
                                "functionId": function_id,
                                "message": json.dumps(tool_to_front, ensure_ascii=False),
                                "reasoningMessage": "",
                                "type": 5,
                            }
                            mq_handler.send_message(answer_queue_message)
                            print(f"[Info] å‘é€å·¥å…·è°ƒç”¨å®Œæˆtype5ï¼š{answer_queue_message}")
                        continue
                    elif data_type == "artifact":
                        print(f"[Info] æ”¶åˆ°artifactæ•°æ®ï¼Œå¦‚æœæˆ‘ä»¬è®¾ç½®çš„Streamï¼Œé‚£ä¹ˆè¿™æ¡æ•°æ®éœ€è¦å¿½ç•¥ï¼š{chunk}")
                        #è¯†åˆ«å®ä½“,artifact_contentåº”è¯¥æ˜¯ç©ºçš„ï¼Œä½¿ç”¨æ”¶é›†çš„reponse_contentè¿›è¡Œå®ä½“è¯†åˆ«
                        artifact_content = chunk["text"]
                        if ENTITY_URL:
                            # entities = entity_indentify_extract_match_db(reponse_content)
                            loop = asyncio.get_running_loop()
                            entities = await loop.run_in_executor(None, entity_indentify_extract_match_db,reponse_content)
                            entities_data = entities["data"]
                            diseases = entities_data.get("diseases")
                            drugs = entities_data.get("drugs")
                            # å¦‚æœæ²¡æœ‰ç–¾ç—…å’Œè¯å“ï¼Œåˆ™ä¸è¿›è¡Œè¿”å›
                            if not diseases and not drugs:
                                continue
                            entities_message = {
                                "sessionId": session_id,
                                "userId": user_id,
                                "functionId": function_id,
                                "message": json.dumps(entities_data, ensure_ascii=False),
                                "reasoningMessage": "",
                                "type": 7,
                            }
                            mq_handler.send_message(entities_message)
                            print(f"[Info] å‘é€å®ä½“è¯†åˆ«æ•°æ®(type 7)ï¼š{entities_message}")
                        continue
                    else:
                        print(f"[è­¦å‘Š] æœªçŸ¥çš„chunkç±»å‹ï¼š{data_type}ï¼Œå·²è·³è¿‡")
                        continue

                    mq_handler.send_message(answer_queue_message)
                    if function_id not in [5000, 9001]:
                        time.sleep(0.01)
                except Exception as chunk_error:
                    print("[é”™è¯¯] å¤„ç† chunk æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š", chunk_error)
                    traceback.print_exc()
                    send_error_message(f"å¤„ç†æ•°æ®å—å‡ºé”™ï¼š{chunk_error}")
        except Exception as stream_error:
            print("[é”™è¯¯] æµæ¶ˆè´¹å¤±è´¥ï¼š", stream_error)
            traceback.print_exc()
            send_error_message(f"å¤„ç†æµå‡ºé”™ï¼š{stream_error}")
        finally:
            mq_handler.close_connection()

    asyncio.run(consume())


def handle_rabbit_queue_message(rabbit_message):
    """
    å¤„ç†ä»RabbitMQ é˜Ÿåˆ—æ¥æ”¶åˆ°çš„æ¶ˆæ¯
    """
    # è§£æmqæ¶ˆæ¯
    print(f"å¤„ç†æ¶ˆæ¯handle_rabbit_queue_message: {rabbit_message}")
    session_id = rabbit_message['sessionId']
    user_id = rabbit_message['userId']
    function_id = rabbit_message['functionId']
    messages = rabbit_message['messages']
    convert_messages = parser_message.transform_user_question(messages)
    user_question_dict = convert_messages.pop()
    # è§£æå‡ºæ¥çš„ç”¨æˆ·é—®é¢˜
    user_question = user_question_dict["content"]
    attachment = rabbit_message.get('attachment')
    if not attachment:
        attachment = {}
    assert isinstance(attachment, dict), f"attachmentå­—æ®µå¿…é¡»æ˜¯å­—å…¸: {attachment}"
    tools = attachment.get("tools", [])

    if function_id == 8:
        # Agent RAGçš„é—®ç­”
        wrapper = A2AClientWrapper(session_id=session_id, agent_url=AGENT_URL)
        stream_response = wrapper.generate(user_question=user_question, history=convert_messages, tools=tools, user_id=user_id)
        handle_gpt_stream_response(session_id, user_id, function_id, stream_response)
    else:
        print('ä¸åœ¨è¿›è¡Œå¤„ç†è¿™æ¡æ¶ˆæ¯ï¼Œfunction_id NOT  : ' + str(function_id))
        return

def callback(ch, method, properties, body):
    """
    mqæ¥æ”¶åˆ°æ¶ˆæ¯åçš„å›è°ƒå‡½æ•°ï¼Œå¤šçº¿ç¨‹å¤„ç†
    """
    try:
        # æ¥æ”¶åˆ°mqçš„æ¶ˆæ¯ï¼šè½¬æ¢æˆdict
        print(f"ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        rabbit_message = json.loads(json.loads(body.decode('utf-8')))
        print(f" [ğŸšš] ä»mqæ¥å—åˆ°æ¶ˆæ¯ï¼š{rabbit_message}")
        # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        executor.submit(handle_rabbit_queue_message, rabbit_message)
        # æ‰‹åŠ¨å‘é€æ¶ˆæ¯ç¡®è®¤
        ch.basic_ack(delivery_tag=method.delivery_tag)
    except Exception as e:
        print(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶æ‹’ç»æ¶ˆæ¯å¹¶é‡æ–°å…¥é˜Ÿ
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)


if __name__ == '__main__':
    print("å¼€å§‹ç›‘å¬RabbitMQé˜Ÿåˆ—...")
    start_consumer(callback, auto_ack=False)
