# æ•´ä¸ªæ‰§è¡Œæµç¨‹
a2a_client.py --> main.py(uvicorn) --> agent_executor.py(executeå‡½æ•°) -->agent.py(streamå‡½æ•°)

# A2A å’ŒLanggraph å¤šè½®å¯¹è¯(langgraphæ§åˆ¶è®°å¿†)
ä¿æŒç›¸åŒçš„task_idå’ŒcontextIdå³å¯ï¼Œ ç›¸åŒçš„contextIdä¼ å…¥Langgraphçš„Memory,Memoryæ ¹æ®thread_idåˆ¤æ–­æ˜¯å¦æ˜¯åŒä¸€ä¸ªç”¨æˆ·
[langgraph_memory](..%2Fexample%2Flanggraph_memory)

ä½¿ç”¨Historyæ¥æ§åˆ¶è®°å¿†ï¼Œä»£æ›¿Agentçš„Memory
[history_langgraph.py](..%2Fexample%2Fhistory_langgraph.py)

# tool Context vs tool State
googleçš„ adkæ˜¯tool contextï¼Œtoolå¯ä»¥è¯»å–å’Œæ›´æ”¹stateä¸­çš„ä¿¡æ¯
https://google.github.io/adk-docs/context/#the-different-types-of-context

langgraphæ˜¯ tool state, toolå¯ä»¥è¯»å–å’Œä¿®æ”¹stateä¸­çš„ä¿¡æ¯
https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#short-term-memory

# InjectedState å·¥å…·è·å–stateä¸­ä¿¡æ¯, å¦‚æœå·¥å…·ä¿¡æ¯å†™å…¥stateï¼Œéœ€è¦ä½¿ç”¨Commandå‘½ä»¤
```python
ç¤ºä¾‹1:
from typing import Annotated, NotRequired
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState, create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

class CustomState(AgentState):
    # The user_name field in short-term state
    user_name: NotRequired[str]

@tool
def get_user_name(
    state: Annotated[CustomState, InjectedState]
) -> str:
    """Retrieve the current user-name from state."""
    # Return stored name or a default if not set
    return state.get("user_name", "Unknown user")

# Example agent setup
agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_user_name],
    state_schema=CustomState,
)

# Invocation: reads the name from state (initially empty)
agent.invoke({"messages": "what's my name?"})


ç¤ºä¾‹2:
from typing_extensions import Annotated
from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState

@tool
def my_tool(query: str, state: Annotated[dict, InjectedState]) -> str:
    info = state.get("user_info", {})
    return f"Hello, {info.get('name', 'guest')}!"

```

# InjectedToolCallId è¿”å›çš„æ¶ˆæ¯å¸¦ä¸Šå·¥å…·çš„id
```python
from typing import Annotated
from langgraph.types import Command
from langchain_core.messages import ToolMessage
from langchain_core.tools import tool, InjectedToolCallId

@tool
def update_user_name(
    new_name: str,
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Update user-name in short-term memory."""
    return Command(update={
        "user_name": new_name,
        "messages": [
            ToolMessage(f"Updated user name to {new_name}", tool_call_id=tool_call_id)
        ]
    })
```

# Command çš„ä¸»è¦ä½œç”¨
Command æ˜¯ä¸€ç§ç‰¹æ®Šè¿”å›å€¼ç±»å‹ï¼ˆè¿”å›å¯¹è±¡ï¼‰ï¼Œå…è®¸å·¥å…·å‡½æ•°åœ¨æ‰§è¡Œç»“æŸå æ˜¾å¼åœ°æ›´æ–° LangGraph çš„ graph stateã€‚
å®ƒæ›¿ä»£äº†ä¼ ç»Ÿåªè¿”å›æ–‡æœ¬çš„æ–¹å¼ï¼Œä½¿å·¥å…·èƒ½å¤ŸåŒæ—¶æ›´æ–°çŠ¶æ€å­—æ®µï¼ˆå¦‚æ–°å¢é”®ï¼‰å¹¶å‘é€æ¶ˆæ¯ç»™ç”¨æˆ·ï¼ˆé€šè¿‡ ToolMessageï¼‰ï¼›
æœ¬è´¨ä¸Šï¼Œæ˜¯å·¥å…·ä¸»åŠ¨å‘ graph æ³¨å…¥ state æ›´æ–°é€»è¾‘çš„æ¥å£ã€‚æ³¨æ„ToolMessageè¿”å›çš„æ•°æ®æ ¼å¼ï¼Œç¬¬ä¸€ä¸ªå‚æ•°content
```python
from langchain_core.messages import ToolMessage
from langchain_core.tools import InjectedToolCallId, tool
from langgraph.types import Command

@tool
def human_assistance(
    name: str, birthday: str,
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    # è°ƒç”¨ human-in-theâ€‘loop æ¥å£ï¼ˆinterruptï¼‰ï¼Œè·å–ç¡®è®¤æˆ–ä¿®æ­£
    ...  
    state_update = {
        "name": verified_name,
        "birthday": verified_birthday,
        "messages": [
            ToolMessage(response, tool_call_id=tool_call_id)
        ],
    }
    return Command(update=state_update)
tool_call_id ç”¨ InjectedToolCallId æ³¨å…¥è°ƒç”¨ IDï¼›
åœ¨ update å­—æ®µä¸­ï¼Œä½ æŒ‡å®šè¦æ’å…¥/è¦†ç›–çš„ state keysï¼Œå¦‚ "name"ã€"birthday"ï¼›
åŒæ—¶ï¼Œ[ToolMessage] å¯ä»¥ç”¨äºæ·»åŠ èŠå¤©è®°å½•å›æ‰§ï¼Œç”±åç»­èŠ‚ç‚¹å¤„ç†å¹¶å±•ç¤ºç»™ç”¨æˆ·ã€‚
```


# MemorySaver æ˜¯ LangGraph çš„ä¸€ä¸ªå†…å­˜æŒä¹…åŒ–å·¥å…·ï¼Œå®ƒä¾èµ–äºä»¥ä¸‹é…ç½®é¡¹æ¥æ­£ç¡®å·¥ä½œ
thread_idï¼šå”¯ä¸€æ ‡è¯†ä¸€ä¸ªå¯¹è¯çº¿ç¨‹ã€‚
checkpoint_nsï¼šå‘½åç©ºé—´ï¼Œç”¨äºç»„ç»‡ä¸åŒçš„æ£€æŸ¥ç‚¹ã€‚
checkpoint_idï¼šç‰¹å®šæ£€æŸ¥ç‚¹çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚


# trim_messages çš„ä½œç”¨ä¸ç”¨æ³•
trim_messages æ˜¯ç”¨æ¥åœ¨æ¶ˆæ¯è¿‡å¤šå¯¼è‡´ä¸Šä¸‹æ–‡è¶…é™æ—¶ï¼Œè£å‰ªå¯¹è¯å†å²çš„å·¥å…·ï¼Œæ”¯æŒæŒ‰ token æ•°æˆ–æ¶ˆæ¯æ¡æ•°è£å‰ªï¼Œå¸¸ç”¨é…ç½®å¦‚ä¸‹ï¼š
strategy="last"ï¼šä¿ç•™æœ€åçš„æ¶ˆæ¯ã€‚
token_counter=count_tokens_approximatelyï¼šä½¿ç”¨è¿‘ä¼¼ token è®¡æ•°å‡½æ•°ï¼Œæ€§èƒ½æ›´å¿«ã€‚
max_tokens=...ï¼šæœ€å¤§ token é™åˆ¶ã€‚
start_on="human"ï¼šç¡®ä¿ä»ç”¨æˆ·æ¶ˆæ¯å¼€å§‹ã€‚
end_on=("human","tool")ï¼šç¡®ä¿ä»¥ç”¨æˆ·æˆ–å·¥å…·æ¶ˆæ¯ç»“æŸã€‚
include_system=Trueï¼šä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚å­˜åœ¨ï¼‰ã€‚
allow_partial å¯é€‰ï¼šå…è®¸æˆªæ–­éƒ¨åˆ†æ¶ˆæ¯å†…å®¹

# pre_model_hook æ’å…¥ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæ¯æ¬¡è°ƒç”¨ LLM ä¹‹å‰æ‰§è¡Œè¿™ä¸ªå‡½æ•°ï¼Œå¯¹æ¶ˆæ¯è¿›è¡Œè£å‰ªã€‚

# å·¥å…·å˜åŒ–ï¼Œå¢å¤šï¼Œå‡å°‘ç­‰ï¼ŒAgentå¿…é¡»é‡æ–°åˆå§‹åŒ–

# é…ç½®æ¨¡å‹çš„ä»£ç†
model = ChatOpenAI(
    model=os.getenv('LLM_MODEL'),
    openai_api_key=os.getenv('OPENAI_API_KEY'),
    temperature=0,
    openai_proxy="http://127.0.0.1:7890"
)

# LangGraph Agentçš„å“åº”æ¨¡å¼
invokeè¡¨ç¤ºä¸€æ¬¡æ€§è¿”å›ç»“æœ
result_state = agent.invoke({"messages": message})

ä½ è¦åœ¨ä½ ç»™å‡ºçš„ ReACT agent ç¤ºä¾‹ä¸­å¯ç”¨ LangGraph çš„ streaming æ¨¡å¼ï¼Œåªéœ€è¦ä½¿ç”¨ `stream`ï¼ˆåŒæ­¥ï¼‰æˆ– `astream`ï¼ˆå¼‚æ­¥ï¼‰æ¥å£ï¼Œå¹¶æŒ‡å®šä¸åŒçš„ `stream_mode`ã€‚ä¸‹é¢æ˜¯å¦‚ä½•ä¿®æ”¹ä½ çš„è„šæœ¬æ¥å®ç°æµå¼è¿”å›çš„æ–¹å¼ï¼š

---

## ğŸ›  åœ¨ ReACT agent ä¸­ä½¿ç”¨ streaming

ä¸‹é¢å±•ç¤ºäº†å¦‚ä½•åœ¨ä½ çš„ä»£ç åŸºç¡€ä¸Šä½¿ç”¨ä¸‰ç§å¸¸ç”¨çš„æµå¼æ¨¡å¼ï¼š

```python
inputs = {"messages": [HumanMessage(content="ä½ å¥½å•Šï¼Œä»‹ç»ä¸‹ä»€ä¹ˆæ˜¯LangGraph")]}
```

### âœ… â€œupdatesâ€ æ¨¡å¼ â€” æµå¼è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ›´æ–°

```python
for chunk in agent.stream(inputs, stream_mode="updates"):
    print(chunk)
```

* æ¯å½“ ReACT agent çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆå¦‚ LLM è¯·æ±‚ã€å·¥å…·è°ƒç”¨ã€æœ€ç»ˆå›ç­”ç­‰ï¼‰æ‰§è¡Œç»“æŸæ—¶ï¼Œè¿”å›è¯¥èŠ‚ç‚¹çš„æ›´æ–°éƒ¨åˆ†ã€‚é€‚åˆè¿½è¸ª agent çš„æ‰§è¡Œæ­¥éª¤ã€‚([langchain-ai.github.io][1], [åšå®¢å›­][2])

### âœ… â€œvaluesâ€ æ¨¡å¼ â€” æµå¼è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹åçš„å®Œæ•´çŠ¶æ€å¿«ç…§

```python
for chunk in agent.stream(inputs, stream_mode="values"):
    print(chunk)
```

* æ¯æ¬¡èŠ‚ç‚¹æ‰§è¡Œå®Œåï¼Œè¿”å›æ•´ä¸ª graph çš„å®Œæ•´ state å¯¹è±¡ã€‚é€‚åˆéœ€è¦å…¨å±€çŠ¶æ€è¿½è¸ªçš„åœºæ™¯ã€‚([DEV Community][3], [CSDN][4])

### âœ… â€œmessagesâ€ æ¨¡å¼ â€” æµå¼è¾“å‡º LLM çš„ token çº§è¾“å‡º

```python
for token, metadata in agent.stream(inputs, stream_mode="messages"):
    print(token)
```

* é€ token è¾“å‡º GPT-4 æˆ–å…¶ä»– LLM çš„å†…å®¹ï¼Œé€‚åˆå³æ—¶æ˜¾ç¤ºæ¨¡å‹æ­£åœ¨â€œæ€è€ƒâ€çš„è¿‡ç¨‹ã€‚([LangChain][5], [langchain-ai.github.io][1])

---

## ğŸ“Œ æ”¯æŒå¤šç§æ¨¡å¼ç»„åˆ

ä½ ä¹Ÿå¯ä»¥åŒæ—¶ä½¿ç”¨å¤šä¸ªæ¨¡å¼ï¼Œä¾‹å¦‚åŒæ—¶è·å–èŠ‚ç‚¹æ›´æ–°å’Œ token æµè¾“å‡ºï¼š

```python
for mode, data in agent.stream(inputs, stream_mode=["updates", "messages"]):
    if mode == "messages":
        token, metadata = data
        print("token:", token)  # LLM è¾“å‡º
    else:  # mode == "updates"
        print("update:", data)
```

* è¿™ç§æ–¹å¼èƒ½åŒæ—¶å±•ç¤º agent æ‰§è¡Œé€»è¾‘å’Œå†…å®¹è¾“å‡ºã€‚([CSDN][4], [langchain-ai.github.io][1])

---

## âš™ ä¸ä½ ä»£ç ç»“åˆç¤ºä¾‹

```python
if __name__ == '__main__':
    inputs = {"messages": [HumanMessage(content="ä½ å¥½å•Šï¼Œä»‹ç»ä¸‹ä»€ä¹ˆæ˜¯LangGraph")]}
    for chunk in agent.stream(inputs, stream_mode=["updates", "messages"]):
        print(chunk)
```

* å¦‚æœä½ ä½¿ç”¨å¼‚æ­¥ç¯å¢ƒï¼Œåˆ™ä½¿ç”¨ `await agent.astream(inputs, stream_mode=...)`ã€‚

---

## ğŸ“š æ€»ç»“å¯¹æ¯”

| æ¨¡å¼         | è¾“å‡ºå½¢å¼                          | å…¸å‹ç”¨é€”               |
| ---------- | ----------------------------- | ------------------ |
| `updates`  | æ¯æ­¥èŠ‚ç‚¹æ›´æ–°ï¼ˆèŠ‚ç‚¹å + è¿”å›å€¼ï¼‰             | æŸ¥çœ‹ agent æ‰§è¡Œè¿‡ç¨‹ï¼ŒèŠ‚çœå¸¦å®½ |
| `values`   | æ¯æ­¥å®Œæ•´ Graph state å¿«ç…§           | å®¡è®¡æˆ–è°ƒè¯•å…¨å±€çŠ¶æ€          |
| `messages` | LLM Model çš„ token-by-token è¾“å‡º | æå‡å¯¹è¯äº¤äº’çš„å®æ—¶æ„Ÿ         |


# curlä½œä¸ºå®¢æˆ·ç«¯æµ‹è¯•
curl -N -X POST http://localhost:10000/ \
  -H "Content-Type: application/json" \
  -d '{
    "id": "c1635d3e-7cd5-4965-a615-14ed8fb28842",
    "jsonrpc": "2.0",
    "method": "message/stream",
    "params": {
      "message": {
        "kind": "message",
        "messageId": "f4181ef28c3a421a9284c633d27ccbcf",
        "metadata": {
          "language": "English"
        },
        "parts": [
          {
            "kind": "text",
            "text": "å¸•é‡‘æ£®çš„æ²»ç–—æ–¹æ¡ˆæœ‰å“ªäº›ï¼Ÿ"
          }
        ],
        "role": "user"
      }
    }
  }'

# æµå¼è¾“å‡ºï¼Œè¿™é‡Œä¹Ÿéœ€è¦æ”¹æˆæµå¼çš„
async for item in self.graph.astream(inputs, config, stream_mode='values'):
